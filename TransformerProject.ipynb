{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PbLKeI2BFZil"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSTX-ZpR5QLP",
        "outputId": "aea837d9-a550-4836-8ddf-e26accf293fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-16 03:26:57--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-05-16 03:26:57 (21.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "vQosQipwG2OC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "input_file = 'input.txt'\n",
        "model_prefix = 'tokenizer'\n",
        "vocab_size = 256\n",
        "\n",
        "# Train model\n",
        "spm.SentencePieceTrainer.train(f'--input={input_file} --model_prefix={model_prefix} --vocab_size={vocab_size} --user_defined_symbols=<n>')"
      ],
      "metadata": {
        "id": "PlvRKhC2HHRW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('tokenizer.model')\n",
        "\n",
        "special_token = '<n>' #Retain new lines\n",
        "text_new = text.replace('\\n', special_token)\n",
        "\n",
        "encoded_text = sp.encode_as_ids(text_new)"
      ],
      "metadata": {
        "id": "5yuhPT6YIsWA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sp.decode_ids(encoded_text).replace(special_token, '\\n')[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB1dXdN0Jatu",
        "outputId": "9f7c1bf7-922a-4404-e7ce-edebc80faf23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = 64\n",
        "train_test_percent = .9\n",
        "batch_size = 32\n",
        "train_data = encoded_text[:int(train_test_percent * len(encoded_text))]\n",
        "test_data = encoded_text[int(train_test_percent * len(encoded_text)):]\n",
        "eval_steps = 200\n",
        "\n",
        "torch.manual_seed(20810581374109)\n",
        "\n",
        "d_model = 256\n",
        "dropout_percent = 0.2\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "learning_rate = 1e-4\n",
        "training_steps = 5000\n"
      ],
      "metadata": {
        "id": "hWKWL0MA6Awt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  #Feedfoward consisting of two linear with a relu inbetween as described in the paper\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.ffn = nn.Sequential(\n",
        "        #We have bias in these following the paper\n",
        "        nn.Linear(d_model, 4*d_model, bias=True), #4x as used in the paper,\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*d_model, d_model, bias=True),\n",
        "        nn.Dropout(dropout_percent)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.ffn(x)\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(d_model, head_size, bias=False)\n",
        "    self.query = nn.Linear(d_model, head_size, bias=False)\n",
        "    self.value = nn.Linear(d_model, head_size, bias=False)\n",
        "    self.dropout = nn.Dropout(dropout_percent)\n",
        "\n",
        "  def forward(self, x):\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "\n",
        "    temp = q @ k.transpose(-2,-1) * (d_model ** -.5)\n",
        "\n",
        "    mask =  torch.tril(torch.ones(context_length, context_length)).to(device)\n",
        "    #We need to mask since it's decoder only transformer and we don't have future context\n",
        "    temp = temp.masked_fill(mask[:x.shape[1], :x.shape[1]] == 0, float('-inf'))\n",
        "    temp = torch.nn.functional.softmax(temp, dim = -1)\n",
        "    temp = self.dropout(temp)\n",
        "    out = temp @ v\n",
        "    return out\n",
        "\n",
        "class MultiheadedAttention(nn.Module):\n",
        "\n",
        "  #MHA based off the paper\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    head_size = d_model // num_heads\n",
        "    self.w_O = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.dropout = nn.Dropout(dropout_percent)\n",
        "    self.multihead = nn.ModuleList([ScaledDotProductAttention(head_size) for _ in range(num_heads)])\n",
        "  def forward(self, x):\n",
        "    tensors = [attention(x) for attention in self.multihead]\n",
        "\n",
        "    out = self.dropout(self.w_O(torch.cat(tensors, dim = -1)))\n",
        "    return out\n",
        "\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "  #Decoder Only Transformer Layer\n",
        "  #Will take input from the embedding throgh a SA layer and then a FF layer\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    #Multiheaded attention\n",
        "\n",
        "    self.mha = MultiheadedAttention()\n",
        "    self.ff = FeedForward()\n",
        "    self.sa_norm = nn.LayerNorm(d_model)\n",
        "    self.ff_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = x + self.mha(self.sa_norm(x))\n",
        "    out = out + self.ff(self.ff_norm(out))\n",
        "    return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  #Transformer\n",
        "  #Will handle the input embeddings and creating the transformer layers\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.tok = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos = nn.Embedding(context_length, d_model)\n",
        "    self.final_linear = nn.Linear(d_model, vocab_size) #Final linear layer that converts to output tokens\n",
        "    self.final_norm = nn.LayerNorm(d_model) #Final normalization before final_linear gets called\n",
        "    self.layers = nn.Sequential(*[TransformerLayer() for _ in range(num_layers)]) #unpacks the list\n",
        "\n",
        "  def forward(self, input, targets=None):\n",
        "    tok = self.tok(input)\n",
        "    pos = self.pos(torch.arange(input.shape[1], device=device))\n",
        "    new_input = tok + pos\n",
        "    new_input = self.layers(new_input)\n",
        "    new_input = self.final_norm(new_input)\n",
        "    new_input = self.final_linear(new_input)\n",
        "    if (targets is not None):\n",
        "      b,t,c = new_input.shape\n",
        "      logits = new_input.view(b*t, c)\n",
        "      targets = targets.view(b*t)\n",
        "      loss = nn.functional.cross_entropy(logits, targets)\n",
        "    else:\n",
        "      loss = None\n",
        "\n",
        "    return new_input, loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def getloss():\n",
        "  model.eval()\n",
        "  out = {}\n",
        "\n",
        "  for split in [True, False]:\n",
        "    losses = []\n",
        "    for _ in range(200):\n",
        "      eval_x, eval_y = create_batch(training=split)\n",
        "      _, loss = model(eval_x, eval_y)\n",
        "      losses.append(loss.item())\n",
        "    if (split):\n",
        "      arr = np.array(losses)\n",
        "      out['train'] = np.mean(arr)\n",
        "    else:\n",
        "      out['val'] = np.mean(np.array(losses))\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "def create_batch(training=True):\n",
        "  data = train_data if training else train_data\n",
        "  #Generate from index 0 to len(data)-context\n",
        "  #as we will take index:index+context_length+1 sized data\n",
        "  ix = torch.randint(len(data) - context_length, (batch_size,))\n",
        "  x = []\n",
        "  y = []\n",
        "  for i in ix:\n",
        "    x.append(data[i:i+context_length])\n",
        "    y.append(data[i+1:i+context_length+1])\n",
        "  x = torch.tensor(x)\n",
        "  y = torch.tensor(y)\n",
        "  x = x.to(device)\n",
        "  y = y.to(device)\n",
        "  return x,y\n",
        "\n",
        "\n",
        "model = Transformer()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(training_steps):\n",
        "  if (iter % 100 == 0) or iter == training_steps - 1:\n",
        "    losses = getloss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    # sample a batch of data\n",
        "  x, y = create_batch(training=True)\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = model(x, y)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "#context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "#print(sp.decode_ids((m.generate(context, max_new_tokens=100)[0].tolist())).replace(special_token, '\\n'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WnQLEo5r7a58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3f647ea-0170-4f92-e46f-47cb2058715b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.88064 M parameters\n",
            "step 0: train loss 5.7033, val loss 5.7028\n",
            "step 100: train loss 4.2330, val loss 4.2375\n",
            "step 200: train loss 3.8143, val loss 3.8171\n",
            "step 300: train loss 3.6535, val loss 3.6512\n",
            "step 400: train loss 3.5487, val loss 3.5458\n",
            "step 500: train loss 3.4884, val loss 3.4882\n",
            "step 600: train loss 3.4372, val loss 3.4248\n",
            "step 700: train loss 3.3790, val loss 3.3874\n",
            "step 800: train loss 3.3441, val loss 3.3455\n",
            "step 900: train loss 3.3081, val loss 3.3099\n",
            "step 1000: train loss 3.2757, val loss 3.2796\n",
            "step 1100: train loss 3.2432, val loss 3.2440\n",
            "step 1200: train loss 3.2210, val loss 3.2181\n",
            "step 1300: train loss 3.1807, val loss 3.1863\n",
            "step 1400: train loss 3.1498, val loss 3.1565\n",
            "step 1500: train loss 3.1206, val loss 3.1172\n",
            "step 1600: train loss 3.0860, val loss 3.0852\n",
            "step 1700: train loss 3.0476, val loss 3.0540\n",
            "step 1800: train loss 3.0113, val loss 3.0191\n",
            "step 1900: train loss 2.9885, val loss 2.9826\n",
            "step 2000: train loss 2.9492, val loss 2.9520\n",
            "step 2100: train loss 2.9288, val loss 2.9295\n",
            "step 2200: train loss 2.9088, val loss 2.9082\n",
            "step 2300: train loss 2.8796, val loss 2.8717\n",
            "step 2400: train loss 2.8519, val loss 2.8573\n",
            "step 2500: train loss 2.8361, val loss 2.8296\n",
            "step 2600: train loss 2.8087, val loss 2.8080\n",
            "step 2700: train loss 2.7781, val loss 2.7870\n",
            "step 2800: train loss 2.7668, val loss 2.7661\n",
            "step 2900: train loss 2.7345, val loss 2.7512\n",
            "step 3000: train loss 2.7269, val loss 2.7260\n",
            "step 3100: train loss 2.7052, val loss 2.7151\n",
            "step 3200: train loss 2.6950, val loss 2.6893\n",
            "step 3300: train loss 2.6722, val loss 2.6725\n",
            "step 3400: train loss 2.6552, val loss 2.6532\n",
            "step 3500: train loss 2.6447, val loss 2.6500\n",
            "step 3600: train loss 2.6287, val loss 2.6269\n",
            "step 3700: train loss 2.6148, val loss 2.6163\n",
            "step 3800: train loss 2.6061, val loss 2.6038\n",
            "step 3900: train loss 2.5882, val loss 2.5891\n",
            "step 4000: train loss 2.5786, val loss 2.5721\n",
            "step 4100: train loss 2.5676, val loss 2.5647\n",
            "step 4200: train loss 2.5544, val loss 2.5503\n",
            "step 4300: train loss 2.5424, val loss 2.5374\n",
            "step 4400: train loss 2.5314, val loss 2.5342\n",
            "step 4500: train loss 2.5233, val loss 2.5143\n",
            "step 4600: train loss 2.5121, val loss 2.5078\n",
            "step 4700: train loss 2.4880, val loss 2.4962\n",
            "step 4800: train loss 2.4869, val loss 2.4809\n",
            "step 4900: train loss 2.4793, val loss 2.4760\n",
            "step 4999: train loss 2.4724, val loss 2.4689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, context, max_new_tokens):\n",
        "  model.eval()\n",
        "  for i in range(max_new_tokens):\n",
        "    contextge = context[:, -context_length:]\n",
        "    logits, loss = model(contextge, None)\n",
        "    prob_dist = nn.functional.softmax(logits, dim=-1)\n",
        "    sample = torch.multinomial(prob_dist[:,-1,:], num_samples=1)\n",
        "    context = torch.cat((context, sample), dim=1)\n",
        "  model.train()\n",
        "  return context\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(sp.decode_ids(generate_text(m, context, max_new_tokens=500)[0].tolist()).replace(special_token, '\\n'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fweT8EJBett_",
        "outputId": "1a41bfaf-c850-41c5-835a-200f209768dd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ⁇ ET:\n",
            "Where, my lord: if you do, no lose:\n",
            "And I take ofk or emselved bushay. usive thy baps,\n",
            "And kind and my came-forrow and me seen speedful\n",
            "Thou may deed\n",
            "Of what had to speak with me: in our need,\n",
            "And high we hear, I'll tail we, speak not whiss,\n",
            "FelfooH's good nature's stale.\n",
            "\n",
            "LUCIO:\n",
            "My Lordio, why, then we speedied,\n",
            "There violer wars in vircaster us;\n",
            "And to an another.\n",
            "Why are thee to begue, the wins stander? O,\n",
            "Have graced hot lives you,\n",
            "So is so clix with queen times hus with me kill\n",
            "As conduaty'd greatends let?\n",
            "We speak joyal 'tign aught joy'd,\n",
            "Yet, with meet he shall sade for your ends:\n",
            "He was I will so tears that do has it well?\n",
            "If is it set that were no out?\n",
            "\n",
            "BENVOLIO:\n",
            "Salk death, is Camillio: thou word, he shall have is,\n",
            "Should foolve by his hence against Sent.\n",
            "\n",
            "Bengger nelds! Chen too, this sent them it son,\n",
            "Nake tellign of this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "len(generate_text(m, context, max_new_tokens=300)[0].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-KJ2kGieuJK",
        "outputId": "0cfc9b6e-a969-4059-b11b-87448cd5a3d8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HScFHHM-_zc6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}